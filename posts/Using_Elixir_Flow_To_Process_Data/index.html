<meta charset="utf-8" emacsmode="-*- markdown -*-">
# Implementing Elixir Flow

January 28th, 2025
By Miguel Iñiguez

# Problem

I have a database containing 400,000 lead records. I need to process a CSV file by reading it row by row to check if each phone number already exists in the database. Afterward, I need to generate a new CSV file with an additional column indicating whether each row is duplicated (exists in the database) or not.
A simple query in the database, such as SELECT * FROM leads WHERE phone = '964480916', takes approximately 5 milliseconds. Therefore, processing 5,000 records sequentially would take 5,000×5 ms=25,000 ms5,000 \times 5 \, \text{ms} = 25,000 \, \text{ms}5,000×5ms=25,000ms, or 25 seconds.

To optimize this process, I plan to use concurrency to reduce the total time required for processing the data.

# Solution using Flow
## What is Flow?
Flow is a library in Elixir designed for handling concurrent and parallel data processing, leveraging Elixir's concurrency capabilities and the Erlang runtime system. It is particularly useful for working with large datasets distributed across multiple nodes or CPU cores.

Key Features:

* Parallel Processing: Allows splitting tasks into multiple processes that can run concurrently to take advantage of multicore CPUs.
* Stream-based Model: Flow uses a model inspired by Apache Spark's data processing approach, processing data in stages like a pipeline.
* Compatible with Enumerable: Works with any data source that implements the Enumerable protocol, such as lists, maps, or streams.
* Memory-efficient: Processes data in configurable batches to avoid loading the entire dataset into memory at once.
* Support for Grouping: Allows grouping of data before processing, which is useful for operations like aggregations.

# Explaning the main module

```elixir
def process_csv_concurrently(file_path, output_path) do
  # Read the file as a stream to process it row by row without loading the entire file into memory.
  file_path
  |> File.stream!()
  
  # Parse the CSV stream into rows, where each row is represented as a list of values.
  |> CSV.parse_stream()
  
  # Create a Flow pipeline from the enumerable (the parsed rows).
  # Flow allows concurrent processing of the rows across multiple cores.
  |> Flow.from_enumerable()
  
  # Apply the `process_row` function to each row in the stream concurrently.
  |> Flow.map(&(process_row(&1)))
  
  # Partition the data for better parallelism. This splits the workload into partitions
  # to ensure efficient concurrent processing.
  |> Flow.partition()
  
  # Collect the results of the Flow processing as a list.
  |> Enum.to_list()
  
  # Generate the final output CSV file with the processed data and write it to the given path.
  |> generate_output_csv(output_path)
end

# Helper function to process a single row.
defp process_row([name, phone]) do
  # Check if a record with the given phone number exists in the database (using Ecto).
  is_duplicate =
    Repo.exists?(
      from l in Lead, where: l.phone == ^phone
    )

  # Return the processed row with the original name and phone,
  # plus a boolean indicating if the record is a duplicate.
  [name, phone, is_duplicate]
end

# Helper function to generate and write the output CSV file.
defp generate_output_csv(data, output_path) do
  # Map over the processed data (row by row).
  # This step doesn't modify the rows but prepares them for output.
  processed_data = Enum.map(data, fn row -> row end)
  
  # Define the headers for the output CSV.
  headers = ["name", "phone", "is_duplicate"]

  # Combine the headers and processed rows into CSV format:
  # 1. Add the headers to the data.
  # 2. Convert each row into a comma-separated string.
  # 3. Join all rows with newlines to form the complete CSV content.
  csv_content =
    [headers | processed_data]
    |> Enum.map(fn row -> Enum.join(row, ",") end)
    |> Enum.join("\n")

  # Write the generated CSV content to the specified output path.
  File.write!(output_path, csv_content)
end

```

# Result

The file containing 5,000 rows was processed in less than 2 seconds. This demonstrates the efficiency of using Elixir's concurrency model through the Flow library, which allowed for parallel processing of rows while interacting with the database. The system efficiently identified duplicate records by querying a database with 400,000 existing records, showcasing its capability to handle large datasets quickly and reliably.

# Repo
https://github.com/miniguez/sandbox_flow

<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>